# ğŸš€ Deploying Llama-3.2-1B-Instruct (vLLM Backend) on NVIDIA Triton Inference Server (GPU)

This documentation explains **how to deploy a lightweight Large Language Model (LLM)** â€” such as **Llama-3.2-1B-Instruct** â€” on the **NVIDIA Triton Inference Server** using the **vLLM backend** for GPU-accelerated and streaming text generation.

The steps cover everything from environment setup to successful model inference using gRPC streaming.

---

## ğŸ§± 1. Prerequisites

Make sure your system meets the following requirements before starting:

- **Windows 10/11** with **WSL2 (Ubuntu)**
- **Docker** installed and configured with GPU support
- **NVIDIA GPU** with Compute Capability â‰¥ 6.0
- **NVIDIA Container Toolkit** installed
- Internet access for pulling Docker images and downloading models

---

## ğŸ§© 2. Project Folder Structure

Organize your files as follows:

```yaml
lora-deployment/
â”‚
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ model_repository/
â”‚   â””â”€â”€ Llama-3.2-1B-Instruct/
â”‚       â””â”€â”€ 1/
|       |   â””â”€â”€ model.josn
|       â””â”€â”€ config.pbtxt
â”œâ”€â”€ vllm_workspace/
â”‚   â””â”€â”€ Llama-3.2-1B-Instruct/
â””â”€â”€ test_client.py


## ğŸ§  3. Create Workspace and Download the Model

Before running Triton, create a folder for vLLM models and download the **TinyLlama** model from Hugging Face.

### ğŸ—‚ï¸ Create the Folder
**Command:**
```bash
mkdir -p vllm_workspace/tiny-llama
â¬‡ï¸ Download the Model from Hugging Face
Commands:

bash
Copy code
pip install huggingface_hub
huggingface-cli download meta-llama/Llama-3.2-1B-Instruct \
  --local-dir Llama-3.2-1B-Instruct \
  --local-dir-use-symlinks False
This will create the following model structure:

pgsql
Copy code
vllm_workspace/tiny-llama/
 â”œâ”€â”€ config.json
 â”œâ”€â”€ tokenizer.json
 â”œâ”€â”€ generation_config.json
 â”œâ”€â”€ model.safetensors
âœ… This folder will later be mounted inside the Triton container as /workspace/tiny-llama.

âš™ï¸ 4. Create Triton Model Configuration
Inside model_repository/Llama-3.2-1B-Instruct/1/config.pbtxt, define the model details:

Model name

Backend type (vLLM)

Input/output definitions

GPU deployment settings

ğŸ“‹ 5. Prepare Requirements File
Create requirements.txt with only the essential dependencies to keep your image lightweight.

Example:

nginx
Copy code
vllm
torch
transformers
accelerate
sentencepiece
huggingface_hub
tokenizers
cmake
numpy
protobuf
pyyaml
ğŸ³ 6. Build the Docker Image
Command:

bash
Copy code
docker build -t triton-vllm-light .
This builds a lightweight Triton container with your model repository and vLLM dependencies.

ğŸ” 7. Authenticate with NVIDIA NGC
Since the Triton images are hosted on NVIDIAâ€™s NGC registry, you must authenticate once.

Commands:

bash
Copy code
docker login nvcr.io
Username: $oauthtoken
Password: <your NGC API key>
Generate your API key here: https://ngc.nvidia.com/setup/api-key

ğŸš€ 8. Run Triton Server (with GPU)
Use the following command to start Triton:

Command:

bash exec.sh
You should see logs ending with:

nginx
Copy code
tiny_llama | 1 | READY
Started HTTPService at 0.0.0.0:8000
Started GRPCInferenceService at 0.0.0.0:8001
âœ… Your model is successfully loaded and running on GPU.

ğŸ§© 9. Test the Model Using gRPC Client
Since vLLM uses streaming inference, you must use a gRPC client (not HTTP).

Commands:

bash
Copy code
pip install tritonclient[grpc]
python test_client.py
Youâ€™ll receive a live streamed response:

vbnet
Copy code
ğŸ§  Partial Output: Artificial
ğŸ§  Partial Output: intelligence
ğŸ§  Partial Output: is the ability...
âœ… Streaming inference complete!
ğŸ“¡ 10. Useful Triton Endpoints
Endpoint Type	URL	Description
HTTP	http://localhost:8000	REST Inference API
gRPC	localhost:8001	gRPC Inference API
Metrics	http://localhost:8002/metrics	Prometheus-compatible metrics
Health Check	http://localhost:8000/v2/health/ready	Returns â€œOKâ€ when Triton is ready

âš™ï¸ 11. Common Issues and Fixes
Problem	Cause	Solution
unable to find backend 'vllm'	Outdated Triton image	Use Triton image 24.09+
decoupled transaction policy	vLLM backend streams output	Use gRPC streaming client instead of HTTP
missing input(s) ['text_input']	Wrong input name in config	Change input name in config.pbtxt and client
501 HTTP not supported	HTTP endpoint doesnâ€™t support streaming	Always use gRPC

ğŸ§  12. Key Learnings
vLLM enables optimized, streaming-based text generation.

Triton Server provides scalable, production-ready model serving.

gRPC streaming is mandatory for LLM backends that use token streaming.

The solution is fully containerized, portable, and GPU-accelerated.

ğŸ¯ Final Result
When setup is complete:

Triton Server loads model tiny_llama and status is READY

Model runs on GPU with streaming inference

gRPC client receives live text generation results

ğŸ§° Tools Used
NVIDIA Triton Inference Server 2.50.0

vLLM 0.5.x

PyTorch 2.4.0

CUDA 12.x

Hugging Face Transformers

Triton gRPC Python Client

ğŸ Run Summary
Step	Description	Command
1	Build Image	docker build -t triton-vllm-light .
2	Run Triton Server	docker run --gpus all -it --rm -p 8000:8000 -p 8001:8001 -p 8002:8002 -v "$(pwd)/model_repository:/models" -v "$(pwd)/vllm_workspace:/workspace" triton-vllm-light
3	Run gRPC Client	python test_client.py

ğŸ§© Architecture Overview
pgsql
Copy code
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ User / Clientâ”‚ ---> â”‚ Triton Inference   â”‚ ---> â”‚ vLLM Backend       â”‚
 â”‚ (Python gRPC)â”‚      â”‚ Server (Docker GPU)â”‚      â”‚ (TinyLlama Model)  â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â–²                        â”‚
         â”‚                        â–¼
     Streaming              GPU-based
     Text Output          Text Generation

ğŸ§¾ Author Information
Author: Tejas Kakade
Project: vLLM Model Deployment on Triton Server
Objective: End-to-end deployment of an open-source LLM (TinyLlama) using NVIDIA Triton with GPU acceleration and streaming inference.
```

ğŸ§  Example Output:

![Model Output](assets/screen_one.png)

  Model_up 
![Model Output](assets/model_up.png)

  Test_output
![Model Output](assets/test_output.png)
---
