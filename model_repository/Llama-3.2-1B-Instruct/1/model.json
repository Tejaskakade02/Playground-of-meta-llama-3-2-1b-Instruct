{
  "model": "/workspace/Llama-3.2-1B-Instruct",
  "dtype": "float16",
  "tensor_parallel_size": 1,
  "gpu_memory_utilization": 0.85,
  "max_model_len": 4096,
  "max_num_seqs": 8
}

